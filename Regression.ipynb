{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DjloVLsm6OE"
      },
      "outputs": [],
      "source": [
        "1 What is Simple Linear Regression?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression is a method to find the relationship between two variables:\n",
        "Independent variable (X): The factor you control or know.\n",
        "\n",
        "\n",
        "Dependent variable (Y): The factor you want to predict or measure.\n",
        "It fits a straight line, Y=mX+cY = mX + cY=mX+c, where:\n",
        "mmm: The slope (how steep the line is).\n",
        "\n",
        "\n",
        "ccc: The intercept (where the line starts on the Y-axis).\n"
      ],
      "metadata": {
        "id": "NdVqtpRBnCcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 What are the key assumptions of Simple Linear Regression"
      ],
      "metadata": {
        "id": "UDXKXaWQnjRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Linear Relationship\n",
        " There must be a straight-line relationship between the independent variable (X) and the dependent variable (Y).\n",
        "\n",
        "Example: As study time (X) increases, grades (Y) improve linearly.\n",
        "\n",
        "2. Independence of Errors\n",
        "The prediction errors (residuals) shouldn't influence each other.\n",
        "\n",
        "Example: If you're predicting house prices in different neighborhoods, one neighborhood’s prediction error shouldn’t affect the next.\n",
        "\n",
        "3. Homoscedasticity (Equal Variance)\n",
        "The spread of prediction errors should be consistent across all levels of X.\n",
        "\n",
        "Example: Predicting income based on years of education—errors in predictions should remain consistent for both high and low levels of education.\n",
        "\n",
        "4. Normality of Residuals\n",
        " The errors (difference between observed and predicted Y) should be normally distributed.\n",
        "\n",
        "Example: When predicting sales based on advertising spend, the errors should mostly cluster around zero.\n"
      ],
      "metadata": {
        "id": "LKmKCg2Rn3-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 What does the coefficient m represent in the equation Y=mX+c?"
      ],
      "metadata": {
        "id": "1MSXneD-oUrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficient mmm represents the slope of the line, which tells us how much YYY (the dependent variable) changes for a one-unit increase in XXX (the independent variable).\n"
      ],
      "metadata": {
        "id": "-FKqMxLAodIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4  What does the intercept c represent in the equation Y=mX+c"
      ],
      "metadata": {
        "id": "fUkbgW_Koz_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intercept ccc is the starting point of the relationship when X=0X = 0X=0. It tells us the value of YYY if no changes occur in XXX.\n"
      ],
      "metadata": {
        "id": "iTl7AsFco8nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5  How do we calculate the slope m in Simple Linear Regression"
      ],
      "metadata": {
        "id": "PnUtx0ddpDVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slope (mmm) in Simple Linear Regression measures how much the dependent variable (Y) changes for a one-unit increase in the independent variable (X). It is calculated as:\n",
        "m=Covariance(X,Y)Variance(X)m = \\frac{\\text{Covariance}(X, Y)}{\\text{Variance}(X)}m=Variance(X)Covariance(X,Y)​\n"
      ],
      "metadata": {
        "id": "iuC6qGE0pGzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 What is the purpose of the least squares method in Simple Linear Regression"
      ],
      "metadata": {
        "id": "zWpgbAuvpkEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It minimizes the sum of squared errors (differences between actual and predicted values) to find the best-fit line.\n"
      ],
      "metadata": {
        "id": "wpOKwE00qYJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7  How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n"
      ],
      "metadata": {
        "id": "xjPUi7N3r4YT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“The general equation for polynomial regression is an extension of linear regression, where we include powers of the independent variable. It’s written as:\n",
        "y=b0+b1x+b2x2+⋯+bnxny = b_0 + b_1x + b_2x^2 + \\dots + b_nx^ny=b0​+b1​x+b2​x2+⋯+bn​xn\n",
        "where nnn is the degree of the polynomial. Each term allows the model to capture non-linear relationships in the data.”\n"
      ],
      "metadata": {
        "id": "o_q35gO-w98b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8 What is Multiple Linear Regression"
      ],
      "metadata": {
        "id": "rwCWDum6w-4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regression (MLR) is a statistical method used to predict the value of a dependent variable based on two or more independent variables. It extends simple linear regression by considering multiple predictors to better explain and forecast outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "fwfyINflxKeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9  What is the main difference between Simple and Multiple Linear Regression?\n"
      ],
      "metadata": {
        "id": "NADUTUUHxRBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"The main difference is the number of predictors. Simple Linear Regression uses one independent variable to predict the outcome, while Multiple Linear Regression uses two or more independent variables. It’s like comparing predicting pizza consumption with just one factor (movies) versus several factors (movies, hunger, time, friends).\""
      ],
      "metadata": {
        "id": "TD3TWNHxxgyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 What are the key assumptions of Multiple Linear Regression"
      ],
      "metadata": {
        "id": "JZU2Vu65xqiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"The key assumptions of Multiple Linear Regression are:\n",
        "\n",
        "Linearity: The relationship between predictors and the outcome is linear.\n",
        "\n",
        "Independence: Residuals are not related.\n",
        "\n",
        "Homoscedasticity: Residuals have constant variance.\n",
        "\n",
        "No Multicollinearity: Predictors are not highly correlated.\n",
        "\n",
        "Normality: Residuals follow a normal distribution.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "1UCA_Y7dx2oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 11 What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model"
      ],
      "metadata": {
        "id": "5KbtT0RoyHlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity occurs when the variance of the errors in a regression model is not constant across all levels of the independent variable(s). This violates a key assumption of regression, leading to unreliable standard errors, biased hypothesis tests, and reduced model efficiency.\n",
        "\n"
      ],
      "metadata": {
        "id": "-SjYjwE_yPVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12 How can you improve a Multiple Linear Regression model with high multicollinearity"
      ],
      "metadata": {
        "id": "fFc6qsAcyQSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High multicollinearity makes it difficult to interpret individual predictor coefficients because the predictors are highly correlated. To improve the model, you can:\n",
        "\n",
        "Remove one of the correlated variables.\n",
        "\n",
        "Combine correlated variables into a single predictor.\n",
        "\n",
        "Use regularization techniques like Ridge or Lasso regression.\n",
        "\n",
        "Perform Principal Component Analysis (PCA) to create uncorrelated predictors."
      ],
      "metadata": {
        "id": "u881ttagyd9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13  What are some common techniques for transforming categorical variables for use in regression models"
      ],
      "metadata": {
        "id": "z_Y29w8lygEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Categorical variables are transformed into numerical formats for regression models using techniques like label encoding, one-hot encoding, binary encoding, frequency encoding, target encoding, and ordinal encoding. Each has trade-offs, and the choice depends on the data and model requirements.\""
      ],
      "metadata": {
        "id": "NrIUC3H0ypaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14  What is the role of interaction terms in Multiple Linear Regression"
      ],
      "metadata": {
        "id": "7RlmOxy5yyJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interaction terms in multiple linear regression show how the effect of one independent variable on the dependent variable depends on the value of another independent variable. They are used to model combined or conditional effects between predictors."
      ],
      "metadata": {
        "id": "izA_7Wtmy8wG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15  How can the interpretation of intercept differ between Simple and Multiple Linear Regression"
      ],
      "metadata": {
        "id": "N2hg65hJzFEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Simple Linear Regression, the intercept is when the predictor (independent variable) is zero. It’s easy to interpret because there’s just one factor.\n",
        "\n",
        "\n",
        "In Multiple Linear Regression, the intercept is when all predictors are zero (e.g., no people, no game, etc.), which may not always make sense in real life.\n"
      ],
      "metadata": {
        "id": "KfsPFTkpzMLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16  What is the significance of the slope in regression analysis, and how does it affect predictions"
      ],
      "metadata": {
        "id": "PwTXUY5NzY6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slope in regression analysis measures the rate of change in the dependent variable (outcome) for every one-unit increase in the independent variable (predictor). It shows the strength and direction of the relationship."
      ],
      "metadata": {
        "id": "1wTAA2okziBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17  How does the intercept in a regression model provide context for the relationship between variables"
      ],
      "metadata": {
        "id": "wlmdgjMYzsQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intercept in a regression model tells us the starting value of the dependent variable when all independent variables are zero. It gives context by showing the baseline level before any changes happen. For example, in predicting taxi fare, the intercept would represent the base charge before any distance is traveled. This helps us understand what the value of Y is when X has no effect yet.\n",
        "\n"
      ],
      "metadata": {
        "id": "-T5LWkzu0HdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18 What are the limitations of using R² as a sole measure of model performance"
      ],
      "metadata": {
        "id": "00FbisGF0P0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R² tells us how much of the variance in the dependent variable is explained by the model. But relying on it alone can be misleading. It doesn’t tell us if the model fits the data well visually, doesn’t reflect overfitting, and can’t handle non-linear relationships. Also, a high R² doesn’t mean the model is good—it could just be memorizing the data, not generalizing well.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "kFOLMSrF0abs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19  How would you interpret a large standard error for a regression coefficient"
      ],
      "metadata": {
        "id": "Q5pqOQEN0gTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The standard error (SE) of a regression coefficient tells us how much that coefficient is likely to vary if we repeat the experiment/sampling many times.\n",
        "\n",
        "A small SE = the estimate is stable and reliable.\n",
        "\n",
        "A large SE = the estimate is unstable and not trustworthy.\n",
        "\n"
      ],
      "metadata": {
        "id": "r9Ux1oBA0nQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20  How can heteroscedasticity be identified in residual plots, and why is it important to address it"
      ],
      "metadata": {
        "id": "NFkeW2OQ1I3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Heteroscedasticity occurs when the spread of residuals increases or decreases with the predicted values — like a cone or funnel shape in a residual plot. It’s important to address because it violates regression assumptions and can lead to unreliable predictions. We can detect it visually in residual plots or test for it statistically, and we often fix it by transforming variables or using robust methods.”\n",
        "\n"
      ],
      "metadata": {
        "id": "Jk3CF-jr1JqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21  What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²"
      ],
      "metadata": {
        "id": "J3jJBJeL1Z0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A high R² with a low Adjusted R² usually means the model includes variables that don’t contribute meaningful information. While R² increases just by adding more predictors, Adjusted R² penalizes irrelevant ones. This suggests the model might be overfitting and includes noise rather than signal.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "kZe0V6QQ1jKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22 Why is it important to scale variables in Multiple Linear Regression"
      ],
      "metadata": {
        "id": "9wFBOtc71r1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We scale variables in multiple linear regression to ensure that all features contribute equally to the model. Without scaling, features with larger numeric ranges can dominate the learning process, even if they aren't more important. Scaling helps stabilize training, especially with gradient descent, and makes coefficient comparison more meaningful. It's essential for model performance and interpretability.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "GhAxv6v_1-WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23  What is polynomial regression"
      ],
      "metadata": {
        "id": "6p_TMDuC1-xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Polynomial regression is an extension of linear regression that models the relationship between the independent variable and the dependent variable as an nth-degree polynomial. While linear regression fits a straight line, polynomial regression fits a curve. It’s useful when the data shows a nonlinear trend, like when predicting growth, temperature changes, or anything that curves over time.\""
      ],
      "metadata": {
        "id": "P0T2hx332LS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24 How does polynomial regression differ from linear regression"
      ],
      "metadata": {
        "id": "U1oQNt0O2USz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression fits a straight line to the data and works best when the relationship between the variables is linear.\n",
        "Polynomial regression, on the other hand, can fit curves by adding higher-degree terms, which makes it suitable for modeling more complex, non-linear relationships.\n",
        "A good way to remember this is: linear is like a straight road, while polynomial is like a winding mountain path — when the path isn’t straight, you need more flexibility.\n",
        "\n"
      ],
      "metadata": {
        "id": "4pTy-7jn2eHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25  When is polynomial regression used"
      ],
      "metadata": {
        "id": "q4zBO53L2pWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is used when the relationship between the independent variable and dependent variable is nonlinear or curved. If a straight line can’t fit the data well — like in cases where the data goes up and down — polynomial regression helps capture that complexity. It's commonly used in fields like physics, biology, and finance when the pattern is not linear."
      ],
      "metadata": {
        "id": "IENh7MLW2yZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26  What is the general equation for polynomial regression"
      ],
      "metadata": {
        "id": "wD6USMVA27Ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Polynomial regression is a type of regression analysis where the relationship between the independent variable x and the dependent variable y is modeled as an nth-degree polynomial. Its general equation is:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "3\n",
        "𝑥\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "+\n",
        "𝜀\n",
        "y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " x+b\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        " +b\n",
        "3\n",
        "​\n",
        " x\n",
        "3\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        " +ε\n",
        "It’s used when data shows a curved trend instead of a straight line.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "xfzBEmTt3Vip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27  Can polynomial regression be applied to multiple variables"
      ],
      "metadata": {
        "id": "y3a_XZq23aLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression can indeed be applied to multiple variables. It’s an extension of linear regression that allows us to fit a curved line to data. While linear regression is for one variable, polynomial regression fits a curve for multiple variables. It’s like mapping a car’s journey based on factors like speed, weather, and road conditions. By considering these multiple factors, we get a more accurate representation of how the variables interact and affect the outcome."
      ],
      "metadata": {
        "id": "upK8_5bb3oSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28  What are the limitations of polynomial regression"
      ],
      "metadata": {
        "id": "PAriJvrS3xdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limitations of Polynomial Regression:\n",
        "\n",
        "Overfitting: A high-degree polynomial can perfectly fit the training data but fails to generalize well to new data. This happens because the model becomes too complex and adapts to noise in the data.\n",
        "\n",
        "Extrapolation Issues: Polynomial models can behave erratically when predicting values outside the range of the training data, leading to unreliable predictions.\n",
        "\n",
        "High Computational Cost: As the degree of the polynomial increases, the complexity of the model increases, leading to longer training times and higher computational requirements.\n",
        "\n",
        "Instability: Higher-degree polynomials may lead to models that are highly sensitive to small changes in the data, causing instability in predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zvm5Hiij322f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29 What methods can be used to evaluate model fit when selecting the degree of a polynomial"
      ],
      "metadata": {
        "id": "VHD_ag434Uta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When selecting the degree of a polynomial, you evaluate the model fit using methods like visual inspection, cross-validation, R-squared, AIC, and MSE. It's about balancing the complexity of the model (degree of the polynomial) with its ability to generalize to new data. Too low a degree may underfit the data, while too high a degree may overfit. The goal is to find a balance where the model fits well without being overly complicated.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xraNussH4hNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30 Why is visualization important in polynomial regression"
      ],
      "metadata": {
        "id": "bFdATtS_4h4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In polynomial regression, visualization is key because it helps us understand how well our model fits the data, especially when the relationship between variables is not straight but curvy. Just like a map guides a driver through curvy roads, visualization shows us the curve of the data, ensuring that the model isn’t overcomplicating things or missing important patterns. It makes it easier to identify underfitting or overfitting, which are critical for selecting the right degree for the polynomial.\n",
        "\n"
      ],
      "metadata": {
        "id": "zK44sx-J4rpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31 How is polynomial regression implemented in Python"
      ],
      "metadata": {
        "id": "ZTg29haz4vzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In polynomial regression, we extend the idea of linear regression by allowing the relationship between the independent and dependent variables to be represented as a polynomial, rather than a straight line. In Python, this is done by:\n",
        "\n",
        "Using PolynomialFeatures to transform the data into polynomial form.\n",
        "\n",
        "Fitting a linear regression model to this transformed data.\n",
        "\n",
        "Making predictions and visualizing the curve to see how the data fits.\n",
        "\n",
        "This approach works well when the data’s relationship is curved (like the cake's shape changing with flour) rather than linear."
      ],
      "metadata": {
        "id": "_1-9AVeq460-"
      }
    }
  ]
}